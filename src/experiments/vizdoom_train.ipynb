{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\kkish\\miniconda3\\envs\\vizdoom\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gymnasium \n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "from vizdoom import DoomGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating VizDoom Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(gymnasium.Env):\n",
    "    def __init__(self, render=False,frameskip=4):\n",
    "        super(VizDoomGym, self).__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('../data/scenarios/deadly_corridor.cfg')\n",
    "        self.step_reward =0 \n",
    "        self.frameskip = frameskip\n",
    "\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8)\n",
    "        self.action_space = spaces.Discrete(7)\n",
    "\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        actions = np.identity(7)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        info = None\n",
    "\n",
    "        for _ in range(self.frameskip):\n",
    "            reward = self.game.make_action(actions[action].flatten(), 1)\n",
    "            done = self.game.is_episode_finished()\n",
    "\n",
    "            if self.game.get_state(): \n",
    "                state = self.game.get_state().screen_buffer\n",
    "                state = self.grayscale(state)\n",
    "                \n",
    "                # Reward shaping\n",
    "                game_variables = self.game.get_state().game_variables\n",
    "                health, damage_taken, hitcount, ammo = game_variables\n",
    "                \n",
    "                # Calculate reward deltas\n",
    "                damage_taken_delta = -damage_taken + self.damage_taken\n",
    "                self.damage_taken = damage_taken\n",
    "                hitcount_delta = hitcount - self.hitcount\n",
    "                self.hitcount = hitcount\n",
    "                ammo_delta = ammo - self.ammo\n",
    "                self.ammo = ammo\n",
    "                \n",
    "                reward += damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5\n",
    "                info = ammo\n",
    "            else: \n",
    "                state = np.zeros(self.observation_space.shape)\n",
    "                info = 0 \n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        truncated = False \n",
    "        self.step_reward = total_reward\n",
    "        return np.array(state), total_reward, done, truncated, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'rgb_array':\n",
    "            return self.game.get_state().screen_buffer\n",
    "        elif mode == 'human':\n",
    "            pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        state = self.grayscale(state)\n",
    "\n",
    "        info = {}  \n",
    "\n",
    "        return np.array(state), info\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "env = VizDoomGym(render=True)\n",
    "env_checker.check_env(env)\n",
    "state = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, verbose=1):\n",
    "        super(PrintProgressCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Print stats every `check_freq` calls\n",
    "            step_reward = self.training_env.get_attr('step_reward')[0]\n",
    "            print(f'Step reward at step {self.num_timesteps}: {step_reward}')\n",
    "        return True\n",
    "    \n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "callback = PrintProgressCallback(check_freq=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)\n",
    "env = DummyVecEnv([lambda: env])  \n",
    "\n",
    "# model = PPO(\"CnnPolicy\", env, verbose=1)\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1,buffer_size=10000)\n",
    "model.learn(total_timesteps=10000, callback=callback)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_vizdoom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulating Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('../data/DeadlyCorridor560k')\n",
    "# model = DQN.load('./dqn_dc3_cnn_600k.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 425.3424835205078\n",
      "Episode 2: 3257.7413940429688\n",
      "Episode 3: 2379.436569213867\n",
      "Episode 4: -434.36134338378906\n",
      "Episode 5: 835.3758544921875\n",
      "Episode 6: 3428.6571350097656\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = VizDoomGym(render=True,frameskip=1)\n",
    "num_episodes = 6\n",
    "time.sleep(10)\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset() \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(np.expand_dims(obs, axis=0)) \n",
    "        obs, reward, done,_, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time.sleep(0.01)\n",
    "    print(f'Episode {episode + 1}: {total_reward}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
